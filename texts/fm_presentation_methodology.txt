1
Methodology Overview

This project focused on combining two broad research methodologies:

Thin and Distant Analysis - Most of the analysis and visualizations in this project were inspired by Derek Mueller’s Network Sense, particularly graphics used in the book such as Google Motion Charts and others. By visualizing the data from a birds-eye-view, I sought to uncover patterns and connections invisible while working directly within First Monday scholarship.

LLM Collaboration: This project employed agentic coding and iterative collaboration with large language models to build a comprehensive corpus from First Monday’s digital archives (1996–2025). The resulting dataset includes 2,710 articles across 359 issues, with complete metadata extraction (title, author, publication date, URL, DOI). Later on, agentic coding was used to build out this website structure and all the visualizations contained within.

2
A Brief Primer on Generative AI/LLMs.

It’s only fair to let some text from the AI make it into the final report; below is a summary from ChatGPT 5 for an introductory primer slide on AI:
Generative AI is a type of artificial intelligence that can create new content—like text, images, or code—based on what it has learned from large amounts of data.


Large Language Models (LLMs) are a kind of generative AI focused on language. Examples include ChatGPT and Claude.


LLMs work by predicting what word or phrase should come next in a sentence, allowing them to answer questions, summarize text, or help write essays and code.


LLMs process text in small pieces called tokens (roughly a few words each). Users are allotted tokens through subscriptions (like ChatGPT Plus) or direct pay-per-use in applications.


These models don’t actually “know” facts—they generate responses based on patterns, so they can sometimes make mistakes or include inaccurate details.


In this project, LLMs were used to help collect data, organize information, and assist in building the visualizations and website.
Text generated by ChatGPT (OpenAI, 2025)


3
LLMs Used In The Project

Claude Code 2.0.31: Claude Code is a command line interface (CLI) agentic coding tool with the capability of directly modifying files and code, managing GitHub repositories, and debugging its own code. I started using Claude Code in exploratory projects right after its Windows release on July 11, 2025, v1.0.51. Similar tools include GitHub’s Copilot, but as I already had a subscription to Claude and was comfortable with operating in the terminal, I’ve had no reason to explore alternatives.

ChatGPT 5: OpenAI’s ChatGPT the general purpose LLM most likely readers are familiar with and remains my daily driver LLM for two reasons: I get a lot of free credits by sharing my data with them, and though I prefer Claude in most applications, I frequently run out of Claude tokens while using Claude Code. ChatGPT was used most often in this project in helping surface investigative leads through open ended questions, using generated text as a sounding board through which to refine my own ideas, and creating clean and comprehensive prompts to pass over to Claude Code.

Claude Sonnet 4: I did not use Claude Sonnet 4 much as I was making sure to use nearly all of my allotted tokens on Claude Code. That said, I occasionally used Claude in the same tasks as ChatGPT, particularly when I was unsatisfied with ChatGPT’s responses and wanted a fresh perspective.

4
Technical Stuff

One more slide written by the AI, this time from Claude Code.
<fill this in based on the directories C:\Users\ferra\DevProjects\FirstMondayScraperV2 and C:\Users\ferra\DevProjects\Garrett-Ferrara-GitHubPage. Keep it VERY HIGH LEVEL; explain what tools you used in the most accessible/concise way. Talk about how the AI made most decisions in picking technology, but occasionally offered choices or gave me opportunities to research how I wanted to promote.

5
Graphs/Visualizations

<Draft an example here, I’ll come back and fill this in>