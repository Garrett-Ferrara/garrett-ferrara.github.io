---
title: "LLM Comparative Prompt Analysis"
summary: "Cross-model prompt set evaluation to surface hallucination, policy variance, jailbreak patterns, and reputational risk."
role: "Designer & Analyst"
tags: ["LLM Evaluation", "Risk Analysis", "Prompt Engineering"]
date: 2025-07-30
weight: 3
---

## TODO: Project Overview

Add 3â€“6 sentence overview summarizing the scope, methodology, and key findings of your LLM comparative analysis. Include information about the models evaluated, the prompt sets used, and the primary risk areas identified.

## Scope

TODO: Describe the scope of your analysis
- Which LLM models were evaluated?
- How many prompts were tested?
- What time period does this cover?

## Method

### Models
TODO: List the models tested (e.g., GPT-4, Claude, Gemini, etc.)

### Sampling
TODO: Describe your sampling strategy and prompt selection criteria

### Scoring
TODO: Explain your evaluation framework and how you scored responses

### Variance Detection
TODO: Describe how you identified and measured policy variance across models

## Findings

TODO: Add redacted findings and examples (ensuring no sensitive data is published)

### Key Observations
- TODO: Finding 1
- TODO: Finding 2
- TODO: Finding 3

## Risk Taxonomy

### Hallucination
TODO: Examples and analysis of hallucination patterns

### Defamation Risk
TODO: Instances of models producing defamatory content

### Security & Information Leakage
TODO: Patterns of unintended information disclosure

### Policy Variance
TODO: Analysis of inconsistent enforcement across models

### Jailbreak Susceptibility
TODO: Techniques that successfully bypass model safety guidelines

## Next Steps

TODO: Outline follow-up research and recommendations

---

**Note:** Publish only redacted findings and anonymized examples to protect model operators and users. Include disclaimers about the limitations of your evaluation and the evolving nature of LLM safety.
